{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Démonstration TextCleaner - Air Paradis\n",
    "\n",
    "Démonstration simple et directe des transformations de `TextCleaner` via `advanced_preprocess()`.\n",
    "\n",
    "**Objectif** : Visualiser l'impact de chaque paramètre de preprocessing sur des tweets Air Paradis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 : Setup\n",
    "\n",
    "Imports et chargement de TextCleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anthonythevenin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Ajouter le répertoire src au path\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "from preprocessing.text_cleaner import TextCleaner\n",
    "\n",
    "# Créer une instance de TextCleaner\n",
    "cleaner = TextCleaner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition des tweets d'exemple Air Paradis avec caractéristiques variées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de 10 tweets d'exemple chargés.\n",
      "\n",
      "Aperçu des tweets originaux:\n",
      "1. @AirParadis Can't believe they didn't help! Service was NOT good at all!!!\n",
      "2. I'm SOOO disappointed!!! Won't recommend @AirParadis. TERRIBLE experience!!!\n",
      "3. Flight wasn't bad but crew didn't care. Can't say I'm happy.\n"
     ]
    }
   ],
   "source": [
    "# Tweets d'exemple avec contractions, négations, émotions\n",
    "tweets = [\n",
    "    \"@AirParadis Can't believe they didn't help! Service was NOT good at all!!!\",\n",
    "    \"I'm SOOO disappointed!!! Won't recommend @AirParadis. TERRIBLE experience!!!\",\n",
    "    \"Flight wasn't bad but crew didn't care. Can't say I'm happy.\",\n",
    "    \"@AirParadis LOOOOVE your service!!! Can't say enough good things! AMAZING!!!\",\n",
    "    \"I'm thrilled! You're the BEST airline ever! Won't fly with anyone else!!!\",\n",
    "    \"Great flight! Crew was professional, seats comfortable. Highly recommend!!!\",\n",
    "    \"NOT happy at all. They didn't honor my booking. So RUUUDE!!!\",\n",
    "    \"Service wasn't great. Food was terrible. Wouldn't fly again.\",\n",
    "    \"Can't believe how GOOD @AirParadis is!!! Not a single complaint! PERFECT!!!\",\n",
    "    \"WORST airline EVER!!! Never again! They don't care about customers!!!\"\n",
    "]\n",
    "\n",
    "print(f\"Total de {len(tweets)} tweets d'exemple chargés.\")\n",
    "print(\"\\nAperçu des tweets originaux:\")\n",
    "for i, tweet in enumerate(tweets[:3], 1):\n",
    "    print(f\"{i}. {tweet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 : Expansion des Contractions\n",
    "\n",
    "Transformation des contractions anglaises en formes complètes.\n",
    "\n",
    "**Paramètre** : `expand_contractions` activé dans `advanced_preprocess`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPANSION DES CONTRACTIONS\n",
      "================================================================================\n",
      "\n",
      "Tweet 1:\n",
      "  AVANT : @AirParadis Can't believe they didn't help! Service was NOT good at all!!!\n",
      "  APRÈS : @AirParadis cannot believe they did not help! Service was NOT good at all!!!\n",
      "\n",
      "Tweet 2:\n",
      "  AVANT : I'm SOOO disappointed!!! Won't recommend @AirParadis. TERRIBLE experience!!!\n",
      "  APRÈS : i am SOOO disappointed!!! will not recommend @AirParadis. TERRIBLE experience!!!\n",
      "\n",
      "Tweet 3:\n",
      "  AVANT : I'm thrilled! You're the BEST airline ever! Won't fly with anyone else!!!\n",
      "  APRÈS : i am thrilled! you are the BEST airline ever! will not fly with anyone else!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPANSION DES CONTRACTIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Sélectionner 3 tweets avec contractions\n",
    "sample_tweets = [tweets[0], tweets[1], tweets[4]]\n",
    "\n",
    "for i, tweet in enumerate(sample_tweets, 1):\n",
    "    # Appliquer seulement l'expansion (sans autres transformations)\n",
    "    expanded = cleaner.expand_contractions(tweet)\n",
    "    \n",
    "    print(f\"Tweet {i}:\")\n",
    "    print(f\"  AVANT : {tweet}\")\n",
    "    print(f\"  APRÈS : {expanded}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3 : Gestion des Émotions\n",
    "\n",
    "Normalisation des répétitions et ajout de marqueurs émotionnels.\n",
    "\n",
    "**Paramètre** : `handle_emotions=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GESTION DES ÉMOTIONS\n",
      "================================================================================\n",
      "\n",
      "Tweet 1:\n",
      "  AVANT : I'm SOOO disappointed!!! Won't recommend @AirParadis. TERRIBLE experience!!!\n",
      "  APRÈS : i am sooo caps disappointed excited will not recommend terrible caps experience excited\n",
      "\n",
      "Tweet 2:\n",
      "  AVANT : @AirParadis LOOOOVE your service!!! Can't say enough good things! AMAZING!!!\n",
      "  APRÈS : loooove caps your service excited can not say enough good things amazing caps excited\n",
      "\n",
      "Tweet 3:\n",
      "  AVANT : NOT happy at all. They didn't honor my booking. So RUUUDE!!!\n",
      "  APRÈS : not caps happy at all they did not honor my booking so ruuude caps excited\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GESTION DES ÉMOTIONS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Tweets avec émotions fortes\n",
    "emotion_tweets = [\n",
    "    tweets[1],  # SOOO, TERRIBLE, !!!\n",
    "    tweets[3],  # LOOOOVE, AMAZING, !!!\n",
    "    tweets[6]   # RUUUDE, !!!\n",
    "]\n",
    "\n",
    "for i, tweet in enumerate(emotion_tweets, 1):\n",
    "    # Appliquer advanced_preprocess avec émotions uniquement\n",
    "    processed = cleaner.advanced_preprocess(\n",
    "        tweet,\n",
    "        handle_negations=False,\n",
    "        handle_emotions=True,\n",
    "        remove_stopwords=False,\n",
    "        use_lemmatization=False,\n",
    "        use_stemming=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Tweet {i}:\")\n",
    "    print(f\"  AVANT : {tweet}\")\n",
    "    print(f\"  APRÈS : {processed}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 : Stemming vs Lemmatization\n",
    "\n",
    "Comparaison directe des deux approches de normalisation.\n",
    "\n",
    "**Stemming** : Réduction à la racine (SnowballStemmer)  \n",
    "**Lemmatization** : Forme canonique (WordNetLemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STEMMING vs LEMMATIZATION\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Tweets pour comparer stemming vs lemmatization\n",
    "compare_tweets = [\n",
    "    tweets[5],  # professional, comfortable, recommend\n",
    "    tweets[3],  # service, things, amazing\n",
    "    tweets[7]   # terrible, wouldn't\n",
    "]\n",
    "\n",
    "for i, tweet in enumerate(compare_tweets, 1):\n",
    "    # Avec stemming\n",
    "    stemmed = cleaner.advanced_preprocess(\n",
    "        tweet,\n",
    "        handle_negations=False,\n",
    "        handle_emotions=False,\n",
    "        remove_stopwords=False,\n",
    "        use_stemming=True,\n",
    "        use_lemmatization=False\n",
    "    )\n",
    "    \n",
    "    # Avec lemmatization\n",
    "    lemmatized = cleaner.advanced_preprocess(\n",
    "        tweet,\n",
    "        handle_negations=False,\n",
    "        handle_emotions=False,\n",
    "        remove_stopwords=False,\n",
    "        use_stemming=False,\n",
    "        use_lemmatization=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Tweet {i}:\")\n",
    "    print(f\"  AVANT         : {tweet}\")\n",
    "    print(f\"  STEMMING      : {stemmed}\")\n",
    "    print(f\"  LEMMATIZATION : {lemmatized}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SUPPRESSION DES STOPWORDS\n",
      "================================================================================\n",
      "\n",
      "Tweet 1:\n",
      "  AVANT                : @AirParadis Can't believe they didn't help! Service was NOT good at all!!!\n",
      "  SANS SUPPRESSION     : can not believe they did not help service wa not good at all\n",
      "  AVEC SUPPRESSION     : not believe not help service not good\n",
      "  Réduction : 13 → 7 mots\n",
      "\n",
      "Tweet 2:\n",
      "  AVANT                : Great flight! Crew was professional, seats comfortable. Highly recommend!!!\n",
      "  SANS SUPPRESSION     : great flight crew wa professional seat comfortable highly recommend\n",
      "  AVEC SUPPRESSION     : great flight crew professional seat comfortable highly recommend\n",
      "  Réduction : 9 → 8 mots\n",
      "\n",
      "Tweet 3:\n",
      "  AVANT                : Can't believe how GOOD @AirParadis is!!! Not a single complaint! PERFECT!!!\n",
      "  SANS SUPPRESSION     : can not believe how good is not a single complaint perfect\n",
      "  AVEC SUPPRESSION     : not believe good not single complaint perfect\n",
      "  Réduction : 11 → 7 mots\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUPPRESSION DES STOPWORDS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Tweets avec beaucoup de stopwords\n",
    "stopword_tweets = [\n",
    "    tweets[0],  # \"they didn't\", \"was NOT good at all\"\n",
    "    tweets[5],  # \"was professional\", \"was comfortable\"\n",
    "    tweets[8]   # \"how GOOD\", \"Not a single\"\n",
    "]\n",
    "\n",
    "for i, tweet in enumerate(stopword_tweets, 1):\n",
    "    # Sans suppression de stopwords\n",
    "    without_removal = cleaner.advanced_preprocess(\n",
    "        tweet,\n",
    "        handle_negations=False,\n",
    "        handle_emotions=False,\n",
    "        remove_stopwords=False,\n",
    "        use_lemmatization=True,\n",
    "        use_stemming=False\n",
    "    )\n",
    "    \n",
    "    # Avec suppression de stopwords\n",
    "    with_removal = cleaner.advanced_preprocess(\n",
    "        tweet,\n",
    "        handle_negations=False,\n",
    "        handle_emotions=False,\n",
    "        remove_stopwords=True,\n",
    "        use_lemmatization=True,\n",
    "        use_stemming=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Tweet {i}:\")\n",
    "    print(f\"  AVANT                : {tweet}\")\n",
    "    print(f\"  SANS SUPPRESSION     : {without_removal}\")\n",
    "    print(f\"  AVEC SUPPRESSION     : {with_removal}\")\n",
    "    print(f\"  Réduction : {len(without_removal.split())} → {len(with_removal.split())} mots\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6 : Suppression des Stopwords\n",
    "\n",
    "Suppression sélective des stopwords (préservation des mots sentimentaux).\n",
    "\n",
    "**Paramètre** : `remove_stopwords=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PIPELINE COMPLET\n",
      "================================================================================\n",
      "\n",
      "Tweet 1:\n",
      "  AVANT : @AirParadis Can't believe they didn't help! Service was NOT good at all!!!\n",
      "  APRÈS : not believe not help service not cap good excited\n",
      "\n",
      "Tweet 2:\n",
      "  AVANT : I'm SOOO disappointed!!! Won't recommend @AirParadis. TERRIBLE experience!!!\n",
      "  APRÈS : sooo cap disappointed excited not recommend terrible cap experience excited\n",
      "\n",
      "Tweet 3:\n",
      "  AVANT : @AirParadis LOOOOVE your service!!! Can't say enough good things! AMAZING!!!\n",
      "  APRÈS : loooove cap service excited not say enough good thing amazing cap excited\n",
      "\n",
      "Tweet 4:\n",
      "  AVANT : Can't believe how GOOD @AirParadis is!!! Not a single complaint! PERFECT!!!\n",
      "  APRÈS : not believe good cap excited not single complaint perfect cap excited\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PIPELINE COMPLET\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Tweets complexes pour démontrer le pipeline complet\n",
    "pipeline_tweets = [\n",
    "    tweets[0],  # Négatif complexe\n",
    "    tweets[1],  # Négatif avec émotions\n",
    "    tweets[3],  # Positif avec émotions\n",
    "    tweets[8]   # Positif avec négation\n",
    "]\n",
    "\n",
    "for i, tweet in enumerate(pipeline_tweets, 1):\n",
    "    # Pipeline complet (sans handle_negations car inutile pour LSTM)\n",
    "    processed = cleaner.advanced_preprocess(\n",
    "        tweet,\n",
    "        handle_negations=False,\n",
    "        handle_emotions=True,\n",
    "        remove_stopwords=True,\n",
    "        use_lemmatization=True,\n",
    "        use_stemming=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Tweet {i}:\")\n",
    "    print(f\"  AVANT : {tweet}\")\n",
    "    print(f\"  APRÈS : {processed}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
