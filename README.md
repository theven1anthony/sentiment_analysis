# Analyse de Sentiments pour Air Paradis

Projet d'analyse de sentiment sur des tweets pour anticiper les potentiels bad buzz sur les r√©seaux sociaux. Ce projet impl√©mente trois approches de machine learning (mod√®le simple, mod√®le avanc√© TensorFlow/Keras, et BERT) avec une infrastructure MLOps compl√®te.

## Table des Mati√®res

- [Installation](#installation)
- [Configuration](#configuration)
- [Donn√©es](#donn√©es)
- [Utilisation](#utilisation)
  - [Entra√Ænement des mod√®les](#entra√Ænement-des-mod√®les)
  - [Interface MLflow](#interface-mlflow)
  - [API de pr√©diction](#api-de-pr√©diction)
- [Architecture](#architecture)
- [Tests](#tests)
- [D√©ploiement](#d√©ploiement)

## Installation

### Pr√©requis
- Python 3.12+
- Git
- Au moins 4GB de RAM pour les mod√®les BERT

### Installation rapide
```bash
# Cloner le projet
git clone <url-du-projet>
cd sentiment_analysis

# Cr√©er l'environnement virtuel
python -m venv venv
source venv/bin/activate  # Sur macOS/Linux
# ou
venv\Scripts\activate     # Sur Windows

# Installer les d√©pendances
pip install -r requirements.txt
```

## Configuration

### 1. Donn√©es
Placez le dataset Sentiment140 dans le dossier `data/` :
```
data/
‚îú‚îÄ‚îÄ training.1600000.processed.noemoticon.csv  # Dataset principal
‚îî‚îÄ‚îÄ raw/                                       # Autres donn√©es brutes
```

### 2. MLflow

**Sans Docker :**
```bash
mlflow ui --host 0.0.0.0 --port 5001
```

**Avec Docker :**
```bash
docker-compose up mlflow
```

Interface accessible sur : http://localhost:5001

## Donn√©es

Le projet utilise le dataset **Sentiment140** contenant 1,6M de tweets √©tiquet√©s :
- **0** : Sentiment n√©gatif
- **1** : Sentiment positif
- **Format** : CSV avec colonnes [sentiment, id, date, query, user, text]
- **R√©partition** : 50% n√©gatif, 50% positif (√©quilibr√©)

### Taille d'√©chantillon

**√âchantillon utilis√© pour ce projet :**
- **50 000 tweets** (apr√®s nettoyage : ~49 827 tweets)
- R√©partition √©quilibr√©e : 50% n√©gatif, 50% positif
- Vocabulaire couvert : ~10-11k mots uniques apr√®s pr√©traitement
- Consommation RAM : < 1GB (confortable pour d√©veloppement)

**Justification :**
- Suffisant pour benchmark et comparaison d'embeddings
- R√©sultats statistiquement stables (√©cart-type < 0.002)
- Temps d'entra√Ænement raisonnables (0.5s simple, 12-20min LSTM)
- Benchmarks ML standards utilisent 50k-400k √©chantillons (IMDB, Sentiment Treebank)

**Pour le mod√®le de production :**
- Augmenter √† 100k-400k tweets pour meilleure g√©n√©ralisation
- Dataset complet (1.6M) possible si ressources suffisantes

## Utilisation

### Entra√Ænement des mod√®les

#### Mod√®le Simple (Baseline)

**Sans Docker :**
```bash
# Avec stemming
python train_simple_model.py --technique=stemming --description="Baseline stemming production"

# Avec lemmatization
python train_simple_model.py --technique=lemmatization --description="Baseline lemmatization"

# Comparaison compl√®te
python train_simple_model.py --technique=both --description="Comparaison techniques preprocessing"
```

**Avec Docker :**
```bash
# Entra√Ænement simple
docker-compose run training python train_simple_model.py --technique=stemming

# Comparaison compl√®te
docker-compose run training python train_simple_model.py --technique=both --description="Comparaison Docker"
```

#### Mod√®les Avanc√©s (TensorFlow/Keras + Embeddings)

**Sans Docker :**
```bash
# Word2Vec avec architecture Dense
python train_word2vec_model.py --technique=stemming --sample-size=50000

# Word2Vec avec LSTM
python train_word2vec_model.py --technique=stemming --with-lstm --sample-size=50000

# FastText avec LSTM
python train_fasttext_model.py --technique=lemmatization --with-lstm

# GloVe Twitter pr√©-entra√Æn√© (recommand√© pour tweets)
python train_glove_model.py --technique=stemming --vector-size=200
python train_glove_model.py --technique=stemming --with-lstm --vector-size=200

# Universal Sentence Encoder
python train_use_model.py --technique=stemming

# BERT fine-tuning
python train_bert_model.py --technique=stemming --epochs=3
```

**Avec Docker :**
```bash
# Word2Vec
docker-compose run training python train_word2vec_model.py --technique=stemming

# FastText
docker-compose run training python train_fasttext_model.py --technique=stemming --with-lstm

# GloVe Twitter (n√©cessite t√©l√©chargement pr√©alable)
docker-compose run training python train_glove_model.py --technique=stemming --vector-size=200

# BERT
docker-compose run training python train_bert_model.py --technique=stemming --epochs=3
```

#### Optimisation d'hyperparam√®tres

**Pr√©requis:** Augmenter la RAM Docker √† 14 GB (Settings ‚Üí Resources ‚Üí Memory)

**Sans Docker :**
```bash
# Optimisation Random Search (20 runs, ~13h)
python optimize_hyperparameters.py --n-runs=20 --sample-size=200000
```

**Avec Docker :**
```bash
# Optimisation Random Search
docker-compose run training python optimize_hyperparameters.py --n-runs=20 --sample-size=200000
```

**Hyperparam√®tres optimis√©s:**
- `vector_size`: [100, 110, 120]
- `lstm_units`: [128, 144]
- `window`: [5, 7]
- `min_count`: [1, 2]
- `dropout`: [0.3, 0.4]
- `recurrent_dropout`: [0.2, 0.3]
- `learning_rate`: [0.0005, 0.001]

**R√©sultats:** Rapport CSV g√©n√©r√© dans `reports/hyperopt_*.csv`

### Interface MLflow

1. **D√©marrer MLflow UI :**
   ```bash
   mlflow ui --host 0.0.0.0 --port 5001
   ```

2. **Naviguer dans l'interface :**
   - **Exp√©rimentations** : Organis√©es par type de mod√®le
   - **Runs** : Chaque entra√Ænement avec ses m√©triques
   - **Mod√®les** : Registre centralis√© des mod√®les
   - **Comparaison** : Comparer les performances visuellement

3. **Tags et filtres disponibles :**
   - `preprocessing_technique` : stemming, lemmatization
   - `model_category` : baseline, advanced, bert
   - `algorithm` : logistic_regression, lstm, bert
   - `description` : Description personnalis√©e

### D√©ploiement en production

#### √âtape 1 : Enregistrer le mod√®le dans Model Registry

**Via MLflow UI (http://localhost:5001):**

1. Acc√©der √† l'exp√©rimentation (ex: `hyperparameter_optimization` ou `word2vec_models_200000_v1`)
2. Filtrer par tag `best_model = true` (si optimisation) ou trier par F1-Score
3. Cliquer sur le run du meilleur mod√®le
4. Onglet **Artifacts** ‚Üí **model** ‚Üí Bouton **Register Model**
5. Cr√©er un nouveau mod√®le ou s√©lectionner un existant:
   - Nom: `w2v_200K_model` (ou `w2v_optimized_model`)
   - Cliquer sur **Register**

**R√©sultat:** Le mod√®le est enregistr√© avec `version=1` (ou version suivante si existant)

#### √âtape 2 : D√©ployer le mod√®le

```bash
# D√©ployer depuis Model Registry vers production
python deploy_model.py --name w2v_200K_model --version 1

# Exemple avec mod√®le optimis√©
python deploy_model.py --name w2v_optimized_model --version 1
```

**Ce script:**
- T√©l√©charge le mod√®le pyfunc complet depuis MLflow Model Registry
- Sauvegarde tous les artefacts (mod√®le + embeddings + pr√©processing) dans `models/production/pyfunc_model/`
- Cr√©e les m√©tadonn√©es dans `models/production/metadata.pkl`
- Le mod√®le est utilisable sans connexion MLflow (local et Azure)

#### √âtape 3 : D√©marrer l'API

L'API charge automatiquement le mod√®le depuis `models/production/pyfunc_model/model/`

**D√©marrer l'API**

**Sans Docker :**
```bash
# Mode d√©veloppement avec rechargement automatique
uvicorn api.main:app --reload --host 0.0.0.0 --port 8000

# Mode production
uvicorn api.main:app --host 0.0.0.0 --port 8000 --workers 4
```

**Avec Docker :**
```bash
# Mode d√©veloppement (avec auto-reload)
docker-compose up api

# En arri√®re-plan
docker-compose up -d api
```

#### Documentation interactive
```bash
# Swagger UI (recommand√©)
http://localhost:8000/docs

# ReDoc (alternative)
http://localhost:8000/redoc
```

#### Utiliser l'API

**Endpoint /predict - Pr√©diction de sentiment**
```bash
curl -X POST "http://localhost:8000/predict" \
     -H "Content-Type: application/json" \
     -d '{"text": "I love this product!"}'

# R√©ponse
{
  "sentiment": 1,
  "confidence": 0.87,
  "text": "I love this product!",
  "prediction_id": "pred_a1b2c3d4",
  "timestamp": "2025-10-07T13:30:00"
}
```

**Endpoint /feedback - Enregistrer un feedback**
```bash
curl -X POST "http://localhost:8000/feedback" \
     -H "Content-Type: application/json" \
     -d '{
       "text": "I love this product!",
       "predicted_sentiment": 0,
       "actual_sentiment": 1,
       "prediction_id": "pred_a1b2c3d4"
     }'

# R√©ponse
{
  "status": "feedback_recorded",
  "message": "Merci pour votre retour",
  "alert_triggered": false,
  "misclassified_count": 1
}
```

**Endpoint /health - Health check**
```bash
curl http://localhost:8000/health

# R√©ponse
{
  "status": "healthy",
  "model_loaded": true,
  "model_type": "logistic_regression",
  "timestamp": "2025-10-07T13:30:00"
}
```

**Endpoint /model/info - Informations du mod√®le**
```bash
curl http://localhost:8000/model/info

# R√©ponse
{
  "model_type": "logistic_regression",
  "technique": "stemming",
  "f1_score": 0.7754,
  "accuracy": 0.7754,
  "training_date": "2025-10-07"
}
```

#### Interface de test Streamlit

L'interface Streamlit permet de tester l'API de mani√®re interactive avec feedback en temps r√©el.

**Pr√©requis :**
1. API en cours d'ex√©cution (voir section pr√©c√©dente)
2. Mod√®le d√©ploy√© dans `models/production/`

**D√©marrer l'interface :**

**Sans Docker :**
```bash
# Depuis la racine du projet
streamlit run interface/app.py
```

**Avec Docker :**
```bash
# Lancer Streamlit + API (recommand√©)
docker-compose up streamlit

# Lancer tout le stack (Streamlit + API + MLflow)
docker-compose up streamlit api mlflow

# En arri√®re-plan
docker-compose up -d streamlit
```

**Interface accessible sur :** http://localhost:8501

**Fonctionnalit√©s :**
- ‚úÖ Analyse de sentiment en temps r√©el
- ‚úÖ Affichage du niveau de confiance
- ‚úÖ Syst√®me de feedback pour corrections
- ‚úÖ Monitoring de l'API et du mod√®le
- ‚úÖ Alertes si 3 erreurs en 5 minutes

**Utilisation :**
1. Entrer un texte dans la zone de saisie
2. Cliquer sur "Analyser le sentiment"
3. Consulter les r√©sultats (sentiment + confiance)
4. Optionnel : Donner un feedback si la pr√©diction est incorrecte

**Note Docker :** Lorsque vous utilisez Docker, l'URL de l'API est automatiquement configur√©e sur `http://api:8000` (communication inter-conteneurs).

Documentation compl√®te : `interface/README.md`

## Architecture

### Structure du projet
```
sentiment_analysis/
‚îú‚îÄ‚îÄ api/                    # API FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # Endpoints de l'API
‚îÇ   ‚îî‚îÄ‚îÄ models.py          # Mod√®les Pydantic
‚îú‚îÄ‚îÄ interface/             # Interface Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ app.py             # Application principale
‚îÇ   ‚îî‚îÄ‚îÄ README.md          # Documentation interface
‚îú‚îÄ‚îÄ data/                   # Datasets
‚îú‚îÄ‚îÄ models/                 # Mod√®les entra√Æn√©s
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/       # Checkpoints pour reprise d'entra√Ænement
‚îÇ   ‚îú‚îÄ‚îÄ staging/           # Mod√®les en test
‚îÇ   ‚îî‚îÄ‚îÄ production/        # Mod√®les d√©ploy√©s
‚îú‚îÄ‚îÄ notebooks/             # Exploration de donn√©es
‚îú‚îÄ‚îÄ reports/               # Rapports d'√©valuation
‚îú‚îÄ‚îÄ src/                   # Code source
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/        # Impl√©mentations d'embeddings
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/        # M√©triques d'√©valuation
‚îÇ   ‚îú‚îÄ‚îÄ models/            # D√©finitions des mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/        # Outils de surveillance
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/     # Pr√©traitement des donn√©es
‚îÇ   ‚îî‚îÄ‚îÄ utils/             # Utilitaires (checkpoint manager, etc.)
‚îú‚îÄ‚îÄ tests/                 # Tests unitaires
‚îî‚îÄ‚îÄ mlruns/               # Donn√©es MLflow
```

### Pipeline MLOps
1. **D√©veloppement** : Notebooks d'exploration
2. **Entra√Ænement** : Scripts param√©tr√©s avec MLflow
3. **√âvaluation** : M√©triques automatiques et rapports
4. **Registre** : Mod√®les versionn√©s dans MLflow
5. **D√©ploiement** : API FastAPI sur cloud
6. **Surveillance** : Monitoring en production

## Tests

```bash
# Tests unitaires
python -m pytest tests/ -v

# Tests avec couverture
python -m pytest tests/ -v --cov=src --cov-report=html

# Tests de qualit√© de code
black src/ tests/ api/      # Formatage
flake8 src/ tests/ api/     # Linting
mypy src/ api/              # V√©rification de types
```

## D√©ploiement

### D√©ploiement local
```bash
# API en mode production (sans Docker)
uvicorn api.main:app --host 0.0.0.0 --port 8000

# Avec Docker Compose (recommand√©)
# Lancer uniquement l'API
docker-compose up api

# Lancer API + MLflow
docker-compose up api mlflow

# Lancer tous les services
docker-compose up

# En arri√®re-plan
docker-compose up -d api

# Arr√™ter les services
docker-compose down
```

**Services disponibles :**
- API FastAPI : http://localhost:8000
- MLflow UI : http://localhost:5001
- Interface Streamlit : http://localhost:8501
- Documentation API : http://localhost:8000/docs

### D√©ploiement cloud (Azure)
```bash
# D√©ploiement automatique via GitHub Actions
# - Push vers la branche main d√©clenche le d√©ploiement
# - Azure App Service (Free tier F1)
# - Pipeline CI/CD avec GitHub Actions
# - Monitoring Azure Application Insights

# URL de production
https://sentiment-api-at2025.azurewebsites.net
```

**Documentation compl√®te du d√©ploiement :**
- Configuration Azure : `docs/azure_configuration.md`
- Secrets GitHub : `docs/github_secrets_azure.md`
- Pipeline CI/CD : `.github/workflows/deploy.yml`

## M√©triques et Monitoring

### M√©triques d'√©valuation
- **Accuracy** : Taux de bonnes classifications
- **F1-Score** : Mesure √©quilibr√©e (m√©trique principale)
- **Pr√©cision/Rappel** : Performance par classe
- **AUC-ROC** : Capacit√© de discrimination
- **Temps d'entra√Ænement** : Performance op√©rationnelle

## R√©sultats du Projet

### Mod√®le de Production : Word2Vec LSTM 200k

**Performance finale :**
```
F1-Score    : 0.7945
Accuracy    : 0.7945
AUC-ROC     : 0.8786
Temps       : 38 min (entra√Ænement)
Latence     : < 50ms/tweet (inf√©rence)
```

**Configuration :**
- Architecture : Word2Vec (100 dim) + Bidirectional LSTM (128 units)
- Pr√©traitement : Stemming
- Dataset : 200 000 tweets Sentiment140
- Entra√Ænement : 10 epochs (early stopping)

**Pourquoi ce mod√®le ?**
- Surpasse BERT 50k (+0.7% F1) avec 6x moins de temps d'entra√Ænement
- D√©ployable sur CPU (pas de GPU requis)
- Meilleure g√©n√©ralisation valid√©e (gap train/val minimal : 0.073)
- Compatible contraintes infrastructure Azure free-tier (F1)

### Approches Test√©es

| Mod√®le | F1-Score | Temps | Commentaire |
|--------|----------|-------|-------------|
| **Word2Vec LSTM 200k** | **0.7945** | 38 min | **Production** ‚úÖ |
| BERT 50k | 0.7892 | 3h48min | Trop co√ªteux |
| Word2Vec LSTM 100k | 0.7846 | 19 min | √âtape validation |
| TF-IDF Baseline | 0.7754 | 0.49s | Excellent baseline |
| Word2Vec LSTM 50k | 0.7653 | 12 min | Manque donn√©es |
| FastText LSTM 50k | 0.7628 | 11 min | Pas d'avantage vs W2V |
| USE 50k | 0.7421 | 77s | Inadapt√© tweets courts |

### M√©thodologie

Le projet a suivi une approche incr√©mentale :

1. **Phase 1 : Benchmark 50k tweets** (6 mod√®les test√©s)
   - Identification baseline TF-IDF (F1=0.7754)
   - Word2Vec LSTM meilleur compromis mod√®les neuronaux
   - BERT champion mais trop co√ªteux

2. **Phase 2 : Progression 50k ‚Üí 100k ‚Üí 200k**
   - Diagnostic : Manque de diversit√© donn√©es sur 50k
   - Validation : Am√©lioration continue (+2.5% puis +1.3% F1)
   - Limite mat√©rielle atteinte : 8.7GB/11.7GB RAM

3. **Phase 3 : Mod√®le de production**
   - Word2Vec LSTM 200k retenu
   - Objectif initial (F1 > 75%) d√©pass√© de +5.9%

**üìÑ Documentation compl√®te :**

Retrouvez l'analyse technique d√©taill√©e, la m√©thodologie compl√®te et tous les r√©sultats d'exp√©rimentations dans :

‚Üí **[blog_article.md](blog_article.md)** (Article technique complet)

**üìä Rapports MLflow :**
- Disponibles dans `reports/mlflow_report_*.txt`
- Interface MLflow : http://localhost:5001

---

### Surveillance en production
- **Seuil d'alerte** : 3 pr√©dictions incorrectes en 5 minutes
- **Monitoring** : Azure Application Insights
- **Alertes** : Email/SMS automatiques via Azure Monitor
- **Drift detection** : Surveillance de la qualit√© des pr√©dictions

## Contribution

1. Fork le projet
2. Cr√©er une branche (`git checkout -b feature/nouvelle-fonctionnalite`)
3. Commit (`git commit -m 'Ajout nouvelle fonctionnalit√©'`)
4. Push (`git push origin feature/nouvelle-fonctionnalite`)
5. Cr√©er une Pull Request

## License

Ce projet est d√©velopp√© dans le cadre de la formation OpenClassrooms AI Engineer.

## Support

Pour toute question ou probl√®me :
1. Consulter la documentation dans `/docs`
2. V√©rifier les issues existantes
3. Cr√©er une nouvelle issue avec le template appropri√©

---

**Statut du projet** : En d√©veloppement actif
**Derni√®re mise √† jour** : Voir les commits r√©cents
